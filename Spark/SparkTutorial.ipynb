{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#write formula latex\n",
    "from IPython.display import display, Math, Latex\n",
    "display(Math(r'\\sqrt{a^2 + b^2}'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Python 2.7\n",
    "conda create -n py27 python=2.7 ipykernel\n",
    "# Python 3.5\n",
    "conda create -n py35 python=3.5 ipykernel\n",
    "#R\n",
    "conda install -c r r-essentials\n",
    "#run notebook\n",
    "jupyter nbconvert --execute <notebook>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init(r\"C:\\Users\\irene\\Downloads\\programmi\\WinSpark\\spark-2.4.2-bin-hadoop2.7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import pyspark\n",
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"Linear Regression Model\") \\\n",
    "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "   .getOrCreate()\n",
    "   \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Exploaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['-122.230000', '37.880000', '41.000000', '880.000000', '129.000000', '322.000000', '126.000000', '8.325200', '452600.000000'], ['-122.220000', '37.860000', '21.000000', '7099.000000', '1106.000000', '2401.000000', '1138.000000', '8.301400', '358500.000000']]\n"
     ]
    }
   ],
   "source": [
    "fname=r\"C:\\Users\\irene\\Downloads\\programmi\\WinSpark\\houses\\cal_housing.data\"\n",
    "rdd=sc.textFile(fname)\n",
    "fname=r\"C:\\Users\\irene\\Downloads\\programmi\\WinSpark\\houses\\cal_housing.domain\"\n",
    "header = sc.textFile(fname)\n",
    "header.collect()\n",
    "\n",
    "# Split lines on commas\n",
    "rdd = rdd.map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Inspect the first 2 lines \n",
    "print(rdd.take(2))\n",
    "# Inspect the first line \n",
    "rdd.first()\n",
    "\n",
    "# Take top elements\n",
    "rdd.top(2)\n",
    "from pyspark.sql import Row\n",
    "# Map the RDD to a DF\n",
    "df = sc.parallelize(rdd.top(2)).map(lambda line: Row(longitude=line[0], \n",
    "                              latitude=line[1], \n",
    "                              housingMedianAge=line[2],\n",
    "                              totalRooms=line[3],\n",
    "                              totalBedRooms=line[4],\n",
    "                              population=line[5], \n",
    "                              households=line[6],\n",
    "                              medianIncome=line[7],\n",
    "                              medianHouseValue=line[8])).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+---------+-----------+----------------+------------+-----------+-------------+-----------+\n",
      "| households|housingMedianAge| latitude|  longitude|medianHouseValue|medianIncome| population|totalBedRooms| totalRooms|\n",
      "+-----------+----------------+---------+-----------+----------------+------------+-----------+-------------+-----------+\n",
      "| 126.000000|       41.000000|37.880000|-122.230000|   452600.000000|    8.325200| 322.000000|   129.000000| 880.000000|\n",
      "|1138.000000|       21.000000|37.860000|-122.220000|   358500.000000|    8.301400|2401.000000|  1106.000000|7099.000000|\n",
      "| 177.000000|       52.000000|37.850000|-122.240000|   352100.000000|    7.257400| 496.000000|   190.000000|1467.000000|\n",
      "| 219.000000|       52.000000|37.850000|-122.250000|   341300.000000|    5.643100| 558.000000|   235.000000|1274.000000|\n",
      "| 259.000000|       52.000000|37.850000|-122.250000|   342200.000000|    3.846200| 565.000000|   280.000000|1627.000000|\n",
      "| 193.000000|       52.000000|37.850000|-122.250000|   269700.000000|    4.036800| 413.000000|   213.000000| 919.000000|\n",
      "| 514.000000|       52.000000|37.840000|-122.250000|   299200.000000|    3.659100|1094.000000|   489.000000|2535.000000|\n",
      "| 647.000000|       52.000000|37.840000|-122.250000|   241400.000000|    3.120000|1157.000000|   687.000000|3104.000000|\n",
      "| 595.000000|       42.000000|37.840000|-122.260000|   226700.000000|    2.080400|1206.000000|   665.000000|2555.000000|\n",
      "| 714.000000|       52.000000|37.840000|-122.250000|   261100.000000|    3.691200|1551.000000|   707.000000|3549.000000|\n",
      "| 402.000000|       52.000000|37.850000|-122.260000|   281500.000000|    3.203100| 910.000000|   434.000000|2202.000000|\n",
      "| 734.000000|       52.000000|37.850000|-122.260000|   241800.000000|    3.270500|1504.000000|   752.000000|3503.000000|\n",
      "| 468.000000|       52.000000|37.850000|-122.260000|   213500.000000|    3.075000|1098.000000|   474.000000|2491.000000|\n",
      "| 174.000000|       52.000000|37.840000|-122.260000|   191300.000000|    2.673600| 345.000000|   191.000000| 696.000000|\n",
      "| 620.000000|       52.000000|37.850000|-122.260000|   159200.000000|    1.916700|1212.000000|   626.000000|2643.000000|\n",
      "| 264.000000|       50.000000|37.850000|-122.260000|   140000.000000|    2.125000| 697.000000|   283.000000|1120.000000|\n",
      "| 331.000000|       52.000000|37.850000|-122.270000|   152500.000000|    2.775000| 793.000000|   347.000000|1966.000000|\n",
      "| 303.000000|       52.000000|37.850000|-122.270000|   155500.000000|    2.120200| 648.000000|   293.000000|1228.000000|\n",
      "| 419.000000|       50.000000|37.840000|-122.260000|   158700.000000|    1.991100| 990.000000|   455.000000|2239.000000|\n",
      "| 275.000000|       52.000000|37.840000|-122.270000|   162900.000000|    2.603300| 690.000000|   298.000000|1503.000000|\n",
      "+-----------+----------------+---------+-----------+----------------+------------+-----------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=rdd.map(lambda line: Row(longitude=line[0], \n",
    "                              latitude=line[1], \n",
    "                              housingMedianAge=line[2],\n",
    "                              totalRooms=line[3],\n",
    "                              totalBedRooms=line[4],\n",
    "                              population=line[5], \n",
    "                              households=line[6],\n",
    "                              medianIncome=line[7],\n",
    "                              medianHouseValue=line[8])).toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- households: string (nullable = true)\n",
      " |-- housingMedianAge: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- medianHouseValue: string (nullable = true)\n",
      " |-- medianIncome: string (nullable = true)\n",
      " |-- population: string (nullable = true)\n",
      " |-- totalBedRooms: string (nullable = true)\n",
      " |-- totalRooms: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the data types of all `df` columns\n",
    "df.columns\n",
    "# df.dtypes\n",
    "\n",
    "# Print the schema of `df`\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all from `sql.types`\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Write a custom function to convert the data type of DataFrame columns\n",
    "def convertColumn(df, names, newType):\n",
    "  for name in names: \n",
    "     df = df.withColumn(name, df[name].cast(newType))\n",
    "  return df \n",
    "\n",
    "# Assign all column names to `columns`\n",
    "columns = ['households', 'housingMedianAge', 'latitude', 'longitude', 'medianHouseValue', 'medianIncome', 'population', 'totalBedRooms', 'totalRooms']\n",
    "# Conver the `df` columns to `FloatType()`\n",
    "df = convertColumn(df, columns, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|population|totalBedRooms|\n",
      "+----------+-------------+\n",
      "|     322.0|        129.0|\n",
      "|    2401.0|       1106.0|\n",
      "|     496.0|        190.0|\n",
      "|     558.0|        235.0|\n",
      "|     565.0|        280.0|\n",
      "|     413.0|        213.0|\n",
      "|    1094.0|        489.0|\n",
      "|    1157.0|        687.0|\n",
      "|    1206.0|        665.0|\n",
      "|    1551.0|        707.0|\n",
      "+----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('population','totalBedRooms').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#groupby and count\n",
    "df.groupBy(\"housingMedianAge\").count().sort(\"housingMedianAge\",ascending=False).show()\n",
    "#get statistics\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['households',\n",
       " 'housingMedianAge',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'medianHouseValue',\n",
       " 'medianIncome',\n",
       " 'population',\n",
       " 'totalBedRooms',\n",
       " 'totalRooms']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(households=126.0, housingMedianAge=41.0, latitude=37.880001068115234, longitude=-122.2300033569336, medianHouseValue=4.526, medianIncome=8.325200080871582, population=322.0, totalBedRooms=129.0, totalRooms=880.0),\n",
       " Row(households=1138.0, housingMedianAge=21.0, latitude=37.86000061035156, longitude=-122.22000122070312, medianHouseValue=3.585, medianIncome=8.301400184631348, population=2401.0, totalBedRooms=1106.0, totalRooms=7099.0)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import all from `sql.functions` \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Adjust the values of `medianHouseValue`\n",
    "df = df.withColumn(\"medianHouseValue\", col(\"medianHouseValue\")/100000)\n",
    "\n",
    "# Show the first 2 lines of `df`\n",
    "df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(households=126.0, housingMedianAge=41.0, latitude=37.880001068115234, longitude=-122.2300033569336, medianHouseValue=4.526, medianIncome=8.325200080871582, population=322.0, totalBedRooms=129.0, totalRooms=880.0, roomsPerHousehold=6.984126984126984, populationPerHousehold=2.5555555555555554, bedroomsPerRoom=0.14659090909090908)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import all from `sql.functions` if you haven't yet\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Divide `totalRooms` by `households`\n",
    "roomsPerHousehold = df.select(col(\"totalRooms\")/col(\"households\"))\n",
    "\n",
    "# Divide `population` by `households`\n",
    "populationPerHousehold = df.select(col(\"population\")/col(\"households\"))\n",
    "\n",
    "# Divide `totalBedRooms` by `totalRooms`\n",
    "bedroomsPerRoom = df.select(col(\"totalBedRooms\")/col(\"totalRooms\"))\n",
    "\n",
    "# Add the new columns to `df`\n",
    "df = df.withColumn(\"roomsPerHousehold\", col(\"totalRooms\")/col(\"households\")) \\\n",
    "   .withColumn(\"populationPerHousehold\", col(\"population\")/col(\"households\")) \\\n",
    "   .withColumn(\"bedroomsPerRoom\", col(\"totalBedRooms\")/col(\"totalRooms\"))\n",
    "   \n",
    "# Inspect the result\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-order and select columns\n",
    "df = df.select(\"medianHouseValue\", \n",
    "              \"totalBedRooms\", \n",
    "              \"population\", \n",
    "              \"households\", \n",
    "              \"medianIncome\", \n",
    "              \"roomsPerHousehold\", \n",
    "              \"populationPerHousehold\", \n",
    "              \"bedroomsPerRoom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[46] at RDD at PythonRDD.scala:53\n"
     ]
    }
   ],
   "source": [
    "#DenseVector() function. A dense vector is a local vector that is backed by a double array that represents its entry values\n",
    "# Import `DenseVector`\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# Define the `input_data` \n",
    "input_data = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "print(input_data)\n",
    "# Replace `df` with the new DataFrame\n",
    "df = spark.createDataFrame(input_data, [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|4.526|[129.0,322.0,126....|\n",
      "|3.585|[1106.0,2401.0,11...|\n",
      "|3.521|[190.0,496.0,177....|\n",
      "|3.413|[235.0,558.0,219....|\n",
      "|3.422|[280.0,565.0,259....|\n",
      "|2.697|[213.0,413.0,193....|\n",
      "|2.992|[489.0,1094.0,514...|\n",
      "|2.414|[687.0,1157.0,647...|\n",
      "|2.267|[665.0,1206.0,595...|\n",
      "|2.611|[707.0,1551.0,714...|\n",
      "|2.815|[434.0,910.0,402....|\n",
      "|2.418|[752.0,1504.0,734...|\n",
      "|2.135|[474.0,1098.0,468...|\n",
      "|1.913|[191.0,345.0,174....|\n",
      "|1.592|[626.0,1212.0,620...|\n",
      "|  1.4|[283.0,697.0,264....|\n",
      "|1.525|[347.0,793.0,331....|\n",
      "|1.555|[293.0,648.0,303....|\n",
      "|1.587|[455.0,990.0,419....|\n",
      "|1.629|[298.0,690.0,275....|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=4.526, features=DenseVector([129.0, 322.0, 126.0, 8.3252, 6.9841, 2.5556, 0.1466]), features_scaled=DenseVector([0.3062, 0.2843, 0.3296, 4.3821, 2.8228, 0.2461, 2.5264])),\n",
       " Row(label=3.585, features=DenseVector([1106.0, 2401.0, 1138.0, 8.3014, 6.2381, 2.1098, 0.1558]), features_scaled=DenseVector([2.6255, 2.1202, 2.9765, 4.3696, 2.5213, 0.2031, 2.6851]))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Initialize the `standardScaler`\n",
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "\n",
    "# Fit the DataFrame to the scaler\n",
    "scaler = standardScaler.fit(df)\n",
    "\n",
    "# Transform the data in `df` with the scaler\n",
    "scaled_df = scaler.transform(df)\n",
    "\n",
    "# Inspect the result\n",
    "scaled_df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ml con Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.1340115638008952, 0.14999),\n",
       " (1.4485018834650096, 0.14999),\n",
       " (1.5713396046425587, 0.14999),\n",
       " (1.7496542762527307, 0.283),\n",
       " (1.2438468929500472, 0.366)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = scaled_df.randomSplit([.8,.2],seed=1234)\n",
    "# Import `LinearRegression`\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Initialize `lr`\n",
    "lr = LinearRegression(labelCol=\"label\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the data to the model\n",
    "linearModel = lr.fit(train_data)\n",
    "# Generate predictions\n",
    "predicted = linearModel.transform(test_data)\n",
    "\n",
    "# Extract the predictions and the \"known\" correct labels\n",
    "predictions = predicted.select(\"prediction\").rdd.map(lambda x: x[0])\n",
    "labels = predicted.select(\"label\").rdd.map(lambda x: x[0])\n",
    "\n",
    "# Zip `predictions` and `labels` into a list\n",
    "predictionAndLabel = predictions.zip(labels).collect()\n",
    "\n",
    "# Print out first 5 instances of `predictionAndLabel` \n",
    "predictionAndLabel[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients for the model\n",
    "linearModel.coefficients\n",
    "\n",
    "# Intercept for the model\n",
    "linearModel.intercept\n",
    "\n",
    "# Get the RMSE\n",
    "linearModel.summary.rootMeanSquaredError\n",
    "\n",
    "# Get the R2\n",
    "linearModel.summary.r2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "- The RMSE measures how much error there is between two datasets comparing a predicted value and an observed or known value. The smaller an RMSE value, the closer predicted and observed values are.\n",
    "\n",
    "- The R2 (“R squared”) or the coefficient of determination is a measure that shows how close the data are to the fitted regression line. This score will always be between 0 and a 100% (or 0 to 1 in this case), where 0% indicates that the model explains none of the variability of the response data around its mean, and 100% indicates the opposite: it explains all the variability. That means that, in general, the higher the R-squared, the better the model fits your data.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# https://spark.apache.org/docs/latest/mllib-clustering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
